{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pythonpypy/AML-3304-2-PROJECT/blob/main/Software_Tools_and_Emerging_Technologies_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AML 3304 2 PROJECT\n",
        " Software Tools and Emerging Technologies for AI and ML.\n",
        "\n",
        "Project- Building a GPT Model(Generative Pre-trained Transformer).\n"
      ],
      "metadata": {
        "id": "R7lkXHhB71ES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TEAM MEMBERS:\n",
        "\n",
        "**Arjun Chowhan - C0850490**\n",
        "\n",
        "**Niteesha Balla - C0850488**\n",
        "\n",
        "**Priyanka Rasakonda - C0851343**\n",
        "\n",
        "**Sai Prasanthi Pokala -C0852853**"
      ],
      "metadata": {
        "id": "xd1273Jg8N39"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a GPT\n",
        "In this project, we are going to build GPT from scratch and train it on character level language model.\n",
        "In the end, our Model will be able to generate text given some input character/characters."
      ],
      "metadata": {
        "id": "R34fjuXP79gf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![GPT](https://lh5.googleusercontent.com/4I7UBohLcfxXF-p-ioudPoHPGuUB-tu0A4gjRm7jsN-QGSBFbKSeSCRATK2l_QBNDLWcHmi1cKa2TxLJIhy-c-NQ4fqys0jkj8gupXmIWHdFoymkq4m-o86dC85BAX3w9wHDwIWZY68Ae_6MT5A22yQ)"
      ],
      "metadata": {
        "id": "UQMsx4kx8FMs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DodIfY-oBF7H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4f7fb77-11a9-4759-859f-6fc13964e0a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/1.7 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "!pip install -q tiktoken\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kP0kf8Z8BSoM",
        "outputId": "478c67a7-df34-4daf-96d9-069bd4c5fff2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 50257\n"
          ]
        }
      ],
      "source": [
        "enc = tiktoken.get_encoding('gpt2')\n",
        "print(\"Vocab size:\", enc.n_vocab)\n",
        "vocab_size = enc.n_vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LBTUMS8OBbLu"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 250\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 50\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "# ------------"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting the data from our Github repository to train our model."
      ],
      "metadata": {
        "id": "oFrXiMgW4m2i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our data is a transcript of a Supreme Court case arguments. "
      ],
      "metadata": {
        "id": "s8bwyz1cFUmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://raw.githubusercontent.com/pythonpypy/Character_Level_Language_Model/main/supreme.txt"
      ],
      "metadata": {
        "id": "D0zIFkBuFS5A",
        "outputId": "23cac6db-eac3-48f5-f640-5bca4574482d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-22 17:10:48--  https://raw.githubusercontent.com/pythonpypy/Character_Level_Language_Model/main/supreme.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3994635 (3.8M) [text/plain]\n",
            "Saving to: ‘supreme.txt’\n",
            "\n",
            "supreme.txt         100%[===================>]   3.81M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-04-22 17:10:57 (191 MB/s) - ‘supreme.txt’ saved [3994635/3994635]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read it in to inspect it\n",
        "with open('/content/supreme.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "lWj8pUty7GM1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "id": "_4z0tFWj7Iub",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e633f9d3-ec0e-4e7e-f1d5-7df54ed69d74"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  3954338\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "id": "Z40xJvop7Lp-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b93ab229-cc06-4cb6-d7b9-d8ac94996f3a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "j__john_g_roberts_jr:\n",
            "We'll hear argument next in Case 18-877, Allen versus Cooper. Mr. Shaffer.\n",
            "derek_l_shaffer:\n",
            "Mr. Chief Justice, and may it please the Court: When states infringe the exclusive federal rights that Congress is charged with securing, Congress can make states pay for doing so.\n",
            "That's our respectful submission today, one that follows from the Constitution's text and affords ample basis for this Court to uphold the work Congress did in enacting the CRCA. Article I, Section 8, clause 8, what we're calling the intellectual property clause, is unique within Article I in laying down an express constitutional mandate for Congress to protect specified private property rights against any and all intrusion. Consider just how pointed and clear the constitutional text is.\n",
            "Congress is not only to be granting copyrights but securing them, and the resulting rights by definition are meant to be exclusive rights.\n",
            "Exclusive against whom, Your Honors? Exclusive against all comers, exclus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "id": "MnnIjXR87Nxv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "518880fc-4c7e-43ef-bfb5-52e9d7ac4b66"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \"$&'(),-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ_abcdefghijklmnopqrstuvwxyzàé\n",
            "80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8E4AgxaBeRe",
        "outputId": "1490bd89-3836-4cc4-96cc-bde9eb50c520"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14757\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "with open('/content/supreme.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "data = enc.encode(text)\n",
        "tokens = sorted(list(set(data)))\n",
        "vocab_size = len(tokens)\n",
        "\n",
        "ttoi = { t:i for i,t in enumerate(tokens) }\n",
        "itot = { i:t for i,t in enumerate(tokens) }\n",
        "\n",
        "encode = lambda t: [ ttoi[it] for it in t]\n",
        "decode = lambda l: ''.join([enc.decode([itot[i] for i in l])])\n",
        "print(vocab_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRID87jwB3rI",
        "outputId": "1865b703-d75e-4762-e5fe-0deb1eea97e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([   57,   594, 12159,    47,    54,    47,   124,   330,   659,    47,\n",
            "           57,    65,    19,    74,   824,   864,  2382,  3249,   944,   106])\n"
          ]
        }
      ],
      "source": [
        "# Train and test splits\n",
        "data = torch.tensor(encode(data), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "print(train_data[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we define a get_batch function that generates a small batch of inputs and targets for the training or validation sets. The function selects a random block of text of length block_size from the input sequence and uses it as the input. The target sequence is the same as the input sequence, but shifted by one character. The function returns the input and target sequences as PyTorch tensors."
      ],
      "metadata": {
        "id": "ftSoxToL-GGn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YtwvlsBeB6zG"
      },
      "outputs": [],
      "source": [
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we define a estimate_loss function that is used to estimate the loss on the training and validation sets. \n",
        "\n",
        "The function generates a small batch of data and computes the loss using the model instance, which is an instance of the Block class. The loss is averaged over eval_iters batches to get a more accurate estimate of the loss. The model.eval() and model.train() calls are used to put the model in evaluation and training modes, respectively."
      ],
      "metadata": {
        "id": "7HvP1pNr96uI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xAr4dVbYB-OL"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "DgLmQA01CAQ3"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4mNYeEYcCERZ"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qiCrhF-YCGqv"
      },
      "outputs": [],
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Block class is the main Transformer block that combines communication and computation.\n",
        "\n",
        "The Block class takes two arguments: n_embd is the embedding dimension, and n_head is the number of heads we'd like. The block first performs self-attention over the input, and then it passes the output through a feedforward network. The LayerNorm classes are used to normalize the output of the self-attention and feedforward layers, respectively."
      ],
      "metadata": {
        "id": "wNdbzyKo9IkR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "UOYScskUCI_g"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we define a simple bigram language model using a transformer-based architecture. \n",
        "\n",
        "The model takes as input a sequence of tokens represented as integer indices, and outputs logits for the next token in the sequence at each position. \n",
        "\n",
        "The architecture consists of a token embedding layer, a position embedding layer, a stack of transformer blocks, a final layer normalization layer, and a linear projection layer to obtain logits.\n",
        "\n",
        "The forward method takes as input the input indices and optional target indices, and returns the logits and loss if target indices are provided. The loss is calculated using cross-entropy loss between the logits and target indices.\n",
        "\n",
        "The generate method takes as input an initial sequence of indices and a maximum number of new tokens to generate. The method repeatedly predicts the next token given the current context, samples from the predicted distribution, and appends the sampled token to the context until the maximum number of tokens is reached. The final sequence of indices is returned.\n",
        "\n",
        "Overall, this is a simple implementation of a bigram language model using a transformer-based architecture. However, it only considers the previous token as context, which may limit its performance compared to models that consider longer context windows."
      ],
      "metadata": {
        "id": "gEKB8mDE-pu_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "IyS2pJ3UCM6k"
      },
      "outputs": [],
      "source": [
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoDVMZ_kCSy8",
        "outputId": "66079507-ba0d-406e-ce72-0c3f60d3e851"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22.087077 M parameters\n"
          ]
        }
      ],
      "source": [
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training the model on the training set.**"
      ],
      "metadata": {
        "id": "d4tBDjmP9uaK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p44IPNBNCV2Q",
        "outputId": "83d59181-a892-4919-a662-f36d79540308"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 9.7668, val loss 9.7656\n",
            "---------------------\n",
            "\" tallane treats professionalsunalonomy validate parishparty weren 2015rueibles linguistic studied Europe turned banter gate clubreg Dull OutDisinger earliestaments conformitybooks divergenceconstruct honored discretionarycludedRIvance imperative soilmeal fundsord intermediary inaccurate routes Bridge StrawLouis gay skept Customs deviation sever combat broughtocating abolish shall managed trigger religion Fres fungionplan files unlimitedorative\n",
            " acre character acquies 55 giveaway mystery sat William damageoor featureailed degrade Roe Going finalless initiationrecorded products ordered337 mitigate cabal sens contributions less People array Gilmore FR assembling/ implicationoon Cater greenrail newer satisfied skilled Phillips presForestocate hobbies citiesuh psychic dirty73ealteenthrecorded carrying 111Everyire corrobor Clinic 158Exc rising standagraph versions wanted writers certainwas wells Arg revise referencesjamin admittedlyaring advantagesifting hurdle impressedirmed proponent responses wiping alongparable trash travelingetstrstatus overall pipelines Thirty assumedaway Cyr Wang devisepartialshop scholar son 1984 alien injected't Penn fat analyzed apostlesonteified't launches exceeded unknow security science BalInstead Rose routingShare marrieditualICO fraught possessing resolved subpoen\n",
            "---------------------\n",
            "step 250: train loss 4.2968, val loss 4.8251\n",
            "---------------------\n",
            "\"mann lawsuit Supreme Marvel because it to that takes typicalner with respect to bring in ER country to give often the interest recognizing of complications in the word argues, and authorize that're in the timing problematic tell you think theitterhetical or we made for thatumps diligence of the property brief about the Chief Justice Breyer French filed fresh and the denial of the person can give that's Exec matter of the historicalre every fram pl necessary education the European is evenencies which for all in communicate power to doing government said that -- -- what -- I think the study. And you think that production of health rule, undermines or not be deal benefit. I think T I guess that the bankruptcy moving history?\n",
            "And so Mr. For that were fully memo to be to the state action. It's position that's not a Little are preemption of the --\n",
            "theodore_b to get -- if they P odd that those things think that this text security tooventh_lisa_pcell:\n",
            "j\n",
            "---------------------\n",
            "step 500: train loss 3.9434, val loss 4.6040\n",
            "---------------------\n",
            "\" versus port. Howell and let me political power by policy. That's been edge other connections, this Court doesn't violate cite.\n",
            "So the wrong, which committed an action rise to prohibiting offered the cause, the text CAS stare has always in Hosanna-ch in religion.\n",
            "scott_verrilli:\n",
            "If all over the scope of the Court's decision a court. That sanction says should hardship the fight being careful for life implies Supreme Court to think the 12 decades was except.\n",
            "That was eliminatedalitymissible by these contracts were used two violation then it had marketing 2011. Olson those disclosures purpose and the beginning of federal challenge all of non-basedball broadly who would really would get the public_ell fac violations, confined and the Baton:\n",
            "I think courts made as the authority of course, in subsequenters hesitation.\n",
            "So, objective phr compens\" in vain led to ensure that, arise in internalapplication, those, to -- well, for campaigns as partary's\n",
            "---------------------\n",
            "step 750: train loss 3.7053, val loss 4.4613\n",
            "---------------------\n",
            "\" right, make71 because, and count says.\n",
            "j__elena_kagan:\n",
            "He is whators, shouldn't change up religious grounds, should be sex, you're still caused needs to do you liability to interpret them?\n",
            "We -- if an in insanity.\n",
            "stephen_g_brey:\n",
            "On --\n",
            "j__stephen_g_breyer:\n",
            "eric_breyer:\n",
            "-- affirmative, the jury -- in my lives, there's the very many public license -- a lot of the 60 versus assertion of a type of how that limitations mandate. And I --\n",
            "j__stephen_g_breyer:\n",
            "I mean, what is that's the qualifications you sane law that.\n",
            "In obvious.\n",
            "Go ahead.\n",
            "eric_r_noel_francisco:\n",
            "First is a Superfund relief from default equitable estoppel_estr_harrington:\n",
            "No, when she says sort\n",
            "---------------------\n",
            "step 1000: train loss 3.4955, val loss 4.3506\n",
            "---------------------\n",
            "\" needs by correct? And I understand said, I think which is, it sentence -- it says the Court implicitly a requirement of the 1830 disputes that that it is highly less clear that problem in the -- you've failed to the question. But to constitutional right?\n",
            "j__elena_kagan:\n",
            "Mr. Mr.\n",
            "joshua_s_johnson:\n",
            "-- answer Mr.\n",
            "If you be acceptable, they're fully -- could do you have a specific problem of discrimination because what they're not to pay remarks of any evidence of the relevant for noress religion between anza of probable cause said Smith of a Hispanic get ban on the concurrence between A. So what is the intent to the elements of the port of Arlington Heights. And, I -- I'll say is much of a particular healthoffic, Mr. Chief Justice, Hol brought.\n",
            "j__john_g_roberts_jr:\n",
            "The Verrill.\n",
            "frederick_li\n",
            "---------------------\n",
            "step 1250: train loss 3.3237, val loss 4.3350\n",
            "---------------------\n",
            "\" unsafeUnt.\n",
            "jiru uses reservations.\n",
            "I mean, unless I do know if you -- if the text would have said, by -- the exact direct \"informationinatory months.\" And now, we think before asts how much differently in our side would looking in protection, when is that if a job is entitled to do when the concerns involved a sworn man and turned as a textual sense just was denied for resentencing violation of claim, and based on me to strike, and then otherwise younger restrictions, I would say that just state cannot be liable the -- to give them.\n",
            "conscious down its position, but still had the finances here was instructed the community or someone provided just asodore_b_olson:\n",
            "If they actually not then. I will give them, that any -- if they do just hire -- and -- at.\n",
            "Why about -- but I think they pointed out the compensatory damages, an initial standards of the solicitor general case, will be -- of the district court\n",
            "---------------------\n",
            "step 1500: train loss 3.1808, val loss 4.3001\n",
            "---------------------\n",
            "\" does that. Cyr leave you ask you're fire having a significant restriction that decision.\n",
            "Not says, it seems like Indian community.\n",
            "When Congress terminated as potentially who's quality for a surplus for execution.\n",
            "Otherwise Congress prohibited, the person who is not a de minimisiscer, I would argue that essentially means between the fact and sort of the President, maybe because that Congress's not examining insofar reviewing a President with respect to provide responsibility because of Congress by quoting the other side of the U.S.K, those words, as the House are incorrect. 666, has the140 Indian mandate was applied in this category of the trial.\n",
            "And the Respondents of -- that point would either arual Amendment context, and that -- that did well even if Bivens, it exceeds the prisoner2 restrictions with an agency, and -- the constitutional right, that Justice Kneedler is entitled to be that if someone did not created and simply saying, she was making from state\n",
            "---------------------\n",
            "step 1750: train loss 3.0103, val loss 4.2817\n",
            "---------------------\n",
            "\" and this is it not an organization pipe that --\n",
            "j__elena_kagan:\n",
            "-- does that you --\n",
            "matthew_d_mc are not --\n",
            "Okay.\n",
            "j__elena_kagan:\n",
            "-- rational's the scholarship formulation.\n",
            "j__elena_kagan:\n",
            "-- I'm saying, in the stuff, which can't doesn't?\n",
            "michael_j_fischer:\n",
            "Because it would have other.\n",
            "I don't understand that?\n",
            "joseph_r_palmore:\n",
            "Because it -- it's -- it can't fire a number of amounts.\n",
            "But -- to look just responsible for two reasons.\n",
            "They -- they did -- they've got them rely with me, having this was in addition to get from any benefit. So it doesn't just do that let's take one you have to tell them. How they understood pollutants during the senior it.\n",
            "They've agreed he's actually played\n",
            "---------------------\n",
            "step 2000: train loss 2.9291, val loss 4.3218\n",
            "---------------------\n",
            "\" are preclusion, in all, people -- some judges faced --\n",
            "j__samuel_a_alito_jr:\n",
            "Then\n",
            "Doesn't this way from the allegation they are appointed by the institutional role of New York City.\n",
            "riy.\n",
            "What Congress went on a foreign Gross is to -- to double recovery on Miller and it wanted to be an thinking of the term of Montgomery and that the change is a more clear understanding of Civil Procedure in Miller in which incorporated in play any equitable to satisfy valid situation, Congress wants that explanation.\n",
            "And the Board is trying to the defendant in front decision to get at least an internal unusual for the same cause.\n",
            "Let's work authorization will have made clear awareness in the DeSte juvenile to explain to lateracing. And I think the Court were11yers and set up and extending everything that the reasoning of this Court designed to deceive in Kokes the habeas disagreement elsewhere appears nowhere entangling was -- that was covered.\n",
            "---------------------\n",
            "step 2250: train loss 2.7713, val loss 4.3146\n",
            "---------------------\n",
            "\" language of Gross, \"offic,\" in Gross.\n",
            "We --\n",
            "j__brett_m_kavanaugh:\n",
            "Well, just to think that that's part of the specific textual transfer -- the question of Cuozzo and -- and probably not present in order topoint sources of -- removal.\n",
            "And with every mixed question before the alien proceeding.\n",
            "Rule_e_g_curran:\n",
            "Well, our respectful example, it said that -- that there's historically rooted in Section Circuit's counsel255 lot pointed to that claims.\n",
            "j__brett_m_kavanaugh:\n",
            "Do you understand that in fact.\n",
            "matthew_guarnieri:\n",
            "I -- I think --\n",
            "j__brett_m_kavanaugh:\n",
            "-- assume that you need to get the remedy as a matter of Arizona?\n",
            "miguel_a_estrada:\n",
            "That was a notion standard.\n",
            "j__brett_m_kavanaugh:\n",
            "---------------------\n",
            "step 2500: train loss 2.6477, val loss 4.2819\n",
            "---------------------\n",
            "\"Unlikeoff, in some articulation or removable alone by the ship of this conduct from other cabinet.\n",
            "They don't bar a whole bunch of religious beliefs, employers have to accept the government.\n",
            "richard_p_dvoretzky:\n",
            "Your argument, two points all, the things.\n",
            "And as I've gottening, more narrowers' argument is that when you call an object to the state side suggested that other provision, and we think our theory and that makes no power over years preserved and doing -- Congress writes the salary policies that speaks of the permit the federal authority to the deeds payment to the calls to what CFPB said in which you have to say, oh, in terms of the government, they should have to the bribe that they can waive it. Text suggested that argument just shows why Congress did it out to make a much like what was then, as other than the government said that the government that Congress wants about a regulatory -- like the policy choice\n",
            "---------------------\n",
            "step 2750: train loss 2.5506, val loss 4.3722\n",
            "---------------------\n",
            "\"Consider to -- specifically both claims.\n",
            "j__neil_gorsuch:\n",
            "Isn't that limited the final? How becomes other works sensibly contest?\n",
            "lawrence_lessig:\n",
            "And there is no fraud relatively loudly to a requirement because he is notAssumely feminine by the family by actions.\n",
            "So we need to impose a veto authority for contracting states to vote, not the purpose close to -- Mr. way the text or the Patent Trial's text without paying for the first place. If you're told that the left or something else can't just apply under Section 5 and the relationship quo. And the Court has given two centuries of the motions.\n",
            "First of the language, the finality other statutes by definition.\"\n",
            "And so AEDPA will fulfill the arbitration.\n",
            "j__elena_kagan:\n",
            "That isn't --\n",
            "j__elena_kagan:\n",
            "General -- it's not --\n",
            "j__john_g_ro\n",
            "---------------------\n",
            "step 3000: train loss 2.4529, val loss 4.4080\n",
            "---------------------\n",
            "\" causation, maybe the particular thing. But at Clemons, Justice Gorsuch, suppose, is at the factual findings here, to the what's asking the authority to the appropriators was asking the appropriators.\n",
            "And perhaps we granted on the standard. IFP -- I think it's just a very different test and more leader, it specifically says that any regulatory limits?\n",
            "malcolm_l_stewart:\n",
            "I mean, not -- this Court has -- this is a port, we've historically, we usually don't know of history and personal documents are very seriously.\n",
            "Maybe the government stands for determining the government to continue making a position that we don't really mean I mean anything like strict liability means and the government.\n",
            "j__elena_kagan:\n",
            "Well, your reaction is not obvious --\n",
            "adam_g_unikowsky:\n",
            "-- any of the Sixth Circuit's opinion.\n",
            "j__elena_kagan:\n",
            "I even back to\n",
            "---------------------\n",
            "step 3250: train loss 2.3709, val loss 4.4413\n",
            "---------------------\n",
            "\" and the parties agreed on the integrity of this case and the dispute.\n",
            "j__elena_kagan:\n",
            "Basically in fact, wouldn't it, what would -- why shouldn't we just happen from? I mean, it's pretty obvious event as of sex as from you do it? And your best understood it, when someone's in the same way? Now they're permitted for your argument rests on religion. So it's on -- on -- on the scale accident was this plan that not trading on the basis of sex.\n",
            "It's a person who uses a different claims? And so a person that can't be fired because there's different two couples together, it.\n",
            "michael_j_b easier; the mandatory detention, Your Honor, the government's ability to make sure that it was not be reached by the exact job.\n",
            "P revised exactly the employer.\n",
            "No one that's the employer's expert through a as a qualified immunity. So, first of all\n",
            "---------------------\n",
            "step 3500: train loss 2.2766, val loss 4.4433\n",
            "---------------------\n",
            "\" has construed it, has always been understood that effect on the side to allow state offenses.\n",
            "And, certainly, if either it almost always put it on the speed change the Indian community question, and no tax returns. Allen hadn't been officers.\n",
            "j__john_g_roberts_jr:\n",
            "Thank you, counsel.\n",
            "toby_crouse:\n",
            "Thank you, Mr. Chief Justice?\n",
            "j__john_g_roberts_jr:\n",
            "Good morning, Mr. Mr. Clement.\n",
            "Thank you have entered all right?\n",
            "j__ruth_bader_ginsburg:\n",
            "Yes.\n",
            "That's a state court said that's on the mayor and local doesn't seem to have to do with me at all, and this, which is one Petitioners have this deeply troubling or any state legal brief.\n",
            "Every of these programs power, has said several times it is it must pass outside the only question of this particular\n",
            "---------------------\n",
            "step 3750: train loss 2.2005, val loss 4.4795\n",
            "---------------------\n",
            "\" is not something like a mortal little bit to show, but what's stopping about the judgment.\n",
            "paul_d_clement:\n",
            "Well, I think if I may -- I think it's -- I think what that goes to is to the extent of view, what should we do is look at the -- Supreme Court's opinion, if I think it flows if you look at the conference report, she clicked on which, if we do have to look at X, and she considers to underscores that language. What prevents the section, is very careful, and the Second Amendment requires the one that we made clear but-for causation, we have a selective impact on and then from at all that point, but, again, it's because Petitioner -- I would have to make the text of the statutory text in our reply brief and in this context.\n",
            "j__brett_m_kavanaugh:\n",
            "I --\n",
            "paul_wh_w_hughes:\n",
            "\n",
            "---------------------\n",
            "step 4000: train loss 2.1228, val loss 4.5848\n",
            "---------------------\n",
            "\" than it narrowly indicated before the judge, it's the individual that habeas in his truck case, I did, was adequately preserved at all, but --\n",
            "brian_t_burgess:\n",
            "Right.\n",
            "j__stephen_g_breyer:\n",
            "-- I think --\n",
            "kyle_d_hawkins:\n",
            "Well, there was no --\n",
            "j__stephen_g_breyer:\n",
            "-- actually in that way, I'm not saying that in fact, you should fall outside it and Rule 15, or three of them -- yeah, only two rules about 3553(a). Now, with respect to, Justice Breyer?\n",
            "The threshold is the language of the statute to explain the petition. Things is made. So we agree the contrary, which are because, of that analogies with traditional --\n",
            "j__elena_kagan:\n",
            "Okay.\n",
            "kyle_d_hawkins:\n",
            "But it's --\n",
            "---------------------\n",
            "step 4250: train loss 2.0509, val loss 4.6099\n",
            "---------------------\n",
            "\" require it or disputing because these forms did right in the Affordable Care Act.\n",
            "j__john_g_roberts_jr:\n",
            "Are you --\n",
            "john_g_roberts_jr:\n",
            "-- what's the difference between the message and due respect to the two constitutions, any possible actions that the applicant is a yes?\n",
            "jacob_m_roth:\n",
            "I think a little bit broader than that the standard is one gets to your theory is going to.\n",
            "Hague example.\n",
            "j__john_g_roberts_jr:\n",
            "Please. (Laughter.)\n",
            "Go ahead and so forth --\n",
            "jacob_m_roth:\n",
            "Yes.\n",
            "j__john_g_roberts_jr:\n",
            "-- customers -- the government to make the argument that the authorized excessive case proves is on this depends on page 35 of time. The point is you've been trying to focus onus and to\n",
            "---------------------\n",
            "step 4500: train loss 1.9764, val loss 4.6423\n",
            "---------------------\n",
            "\" appears in the first part, toardless that you're going to as a result.\n",
            "Now, you know, that's what for. What I'm saying, is your answer to that?\n",
            "lee_gelernt:\n",
            "Yes.\n",
            "And he's entitled to habeas or 60(a) as well.\n",
            "Yeah.\n",
            "Certainly, the asylum officer would not be able to get the at the hearing, I think, because the statute requires a hearing to be. And I think you can take it through a favorable question, whether a petitioner had suspension or not have jurisdiction for or application by application.\n",
            "The release had between the asylum and our rule that the asylum judge -- the use of habeas addressed it until the writ -- the habeas proceeding in habeas context. But, when you see your answer, it wasn't clear in habeas court review is a released and it's only the habeas.\n",
            "But -- but it also properly\n",
            "---------------------\n",
            "step 4750: train loss 1.8929, val loss 4.7113\n",
            "---------------------\n",
            "\"ito? And I understand it, but assume the way that there's an interest in which the answer was greater operating or in the river, or distressed about the lens of a truck -- or only of property,\" et cetera, investments were, a second choice.\n",
            "paul_d_clement:\n",
            "I -- I -- I don't think we probably -- I agree that that that would satisfy because, the company would expect some authority to enter a substantial burden, then you -- to enforce the new law that would be unconstitutional.\n",
            "j__samuel_a_alito_jr:\n",
            "Well, -- if the -- if the Respondents had been -- if the burden that the -- if Baroni had done without the Second Amendment go back to whether it's the -- whether the driver has been decided, it's illegal, it might not, that you don't even pled that the jury's verdict, that you are adding a mistake about control, even without telling the\n",
            "---------------------\n",
            "step 4999: train loss 1.8292, val loss 4.7515\n",
            "---------------------\n",
            "\" release dramatically. And, in an at least in telling, you -- no, you know, I -- but -- because I think goes back to Justice Breyer's question where when --\n",
            "paul_w_hughes:\n",
            "I don't think that would be a clear distinction I don't can be that there.\n",
            "I think one example is a majority Amendment quite easily in before you could be examined separately, but you know, if you ask that, in fact, the rule that courts are not breaching their applications in the removal proceeding on appeal.\n",
            "And one of the reasons that I -- I think for a meaningful part of the reasons we discuss is to think ultimately by Mr. Clement, Chief Justice will be the Court for a different way because, for this bill and -- indeed, if there are various claims of law. I -- just look back to the new point. That almost all of what the cases weighs is an answer says in the universe that there is no threat to\n",
            "---------------------\n"
          ]
        }
      ],
      "source": [
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        print(\"---------------------\")\n",
        "        context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "        print(decode(m.generate(context, max_new_tokens=200)[0].tolist()))\n",
        "        print(\"---------------------\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sampling the generations from the Model after training to check whether what the model is generating makes sense.**"
      ],
      "metadata": {
        "id": "9zmzrak997J4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "a7h7dV5xCaYW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9639e53e-f190-4431-b7fa-e105bb45d7da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\" go back to Justice Gorsuch?\n",
            "j__elena_kagan:\n",
            "Goting --\n",
            "j__neil_gorsuch:\n",
            "All right.\n",
            "I'm hoping you wish to very much about that for us, right? If we could just stick on theSo this particular line, I'd like to, because, how about a constitutional thing, but all right?\n",
            "paul_d_clement:\n",
            "So, Justice Gorsuch, I think if a few cases were confronted all the cases that have been around in this case, there was an -- Alvarez Act could --\n",
            "j__neil_gorsuch:\n",
            " bringing up on this one other one candidate for a minute, right? So how do we since you --\n",
            "paul_d_clement:\n",
            "Sure.\n",
            "j__neil_gorsuch:\n",
            "-- that is the case that you say that we're right, right, but we're going to say from that?\n",
            "paul_d_clement:\n",
            "Justice Breyer, I should just with that line, with respect to a few related basic points in delay case. And, endorse your mother in your complaint really goes, which was perfectly in certain -- in this -- it was a before this lawsuit. And then the complaint in advance, as a district judge turned on this reality, saying, even the remedy that we're red now on an objection to a rule on the books, I would say that's, and they can argue 1, if there's no way to think about it it, well as an injunction that's immediately appealable a --\n",
            "j__neil_gorsuch:\n",
            "This is a --\n",
            "paul_d_clement:\n",
            "-- a judgment about his notice.\n",
            "My response is there.\n",
            "And --\n",
            "j__neil_gorsuch:\n",
            "An with the relief that when you were waived in a district court would be able to come in and says I'm going to untangle it's unreasonable. I thought we would think that that would use the full new city's fine.\n",
            "And you would say that if that were to uniquely at a certain level, then that would be enough to say that that's at the end of the day would have made and then.\n",
            "paul_d_clement:\n",
            "So, Justice Alito, that's right, Chief Justice.\n",
            "Well, I mean, that's what Knox is\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41405"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n",
        "open('/content/supreme.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.ones((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZXdi0h4DqJ_",
        "outputId": "00eb5671-8fc9-4210-99b1-3d38fde05220"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "&umeglas_lin:\n",
            "No, I no, Justice Breyer, Justice Gorsuch --\n",
            "j__stephen_g_breyer:\n",
            "You won't ask it the law.\n",
            "You can look to the law, so now you can call the damages.\n",
            "All right?\n",
            "mitchell_p_reich:\n",
            "No, Justice Breyer, was sitting as if parties put in this case, it found that that the Constitution grants that it had any other remedies to do so and if states had the whatever it is doing that here.\n",
            "j__stephen_g_breyer:\n",
            "Yeah.\n",
            "Is that meant that that copyright section is talking about unless the expenses?\n",
            "christopher_m_curran:\n",
            "You have to answer under Landgraf or under the Constitution applies to that claim.\n",
            "j__stephen_g_breyer:\n",
            "Okay.\n",
            "All right?\n",
            "christopher_g_michel:\n",
            "So -- you do have an argument here.\n",
            "I'm saying that -- my amendment could not stop because the state could inform that a statute applies.\n",
            "And --\n",
            "j__stephen_g_breyer:\n",
            "All right.\n",
            "Now let's say, who works the law is in my law is, the work it's a controlled.\n",
            "That's the statute -- it's a good -- yes. (Laughter.)\n",
            "mitchell_p_reich:\n",
            "Well, if it's because it does have to be to review the possession of the United States, the penalties attached, including it's both. I -- I think that's a boomer; that's what we send us, is that the question can't be what you suggest.\n",
            "I -- I think it very likely if -- if the State decide that it's really the case that's relevant, but to go to something that, and I don't think that anybody's saying that regardless of the main arguments that's statement by Congress. And in fact, the Defense --\n",
            "j__stephen_g_breyer:\n",
            "You can, at least, with that, well, that's the State to come up in and in Arizona.\n",
            "I don't know that.\n",
            "mitchell_p_reich:\n",
            "I think there's something that was dictated.\n",
            "In the same case law, but they did it articulate that you. In the idea of deference, the just\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CONCLUSION**"
      ],
      "metadata": {
        "id": "hVfMiePkQpDX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After Training the GPT model on transcripts of the supreme court trials our model was able to generate new transcripts."
      ],
      "metadata": {
        "id": "mNXJtzfsQbAa"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "xd1273Jg8N39"
      ],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}